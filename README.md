# PySpark Data Processing and Analysis
This repository demonstrates how to use PySpark to analyze and process flight data. The project explores various PySpark functionalities for loading, transforming, querying, and analyzing datasets efficiently using both SQL and DataFrame APIs.

### Features
- Data Cleaning: Includes transformations for handling missing values and ensuring data consistency.
- Data Aggregation: Demonstrates how to perform group operations and summarizations using PySpark.
- Data Processing: Implements efficient and scalable techniques for processing large datasets.
- Integration with PySpark: Showcases the integration of PySpark for handling big data workloads in distributed environments.

- Data Loading: Demonstrates how to load CSV data into PySpark DataFrames with schema inference and headers.
- Sorting and Viewing Data: Utilizes sorting and data preview methods to inspect datasets.
- Temporary Views and SQL Queries:
  - Creates temporary SQL views for querying datasets using standard SQL syntax.
  - Examples include GROUP BY, ORDER BY, and aggregate functions like MAX and SUM.
- DataFrame Operations:
  - Performs grouping and aggregation directly using PySpark's DataFrame API.
  - Renames columns and sorts aggregated results efficiently.
- Query Optimization: Leverages the explain() function to analyze query execution plans and optimize operations.

### Getting Started
#### Prerequisites
To run this project, ensure you have the following installed:
- Python 3.8 or higher
- Apache Spark
- Jupyter Notebook
- Required Python packages

### Requirements
The project requires the following Python packages:
- PySpark
- Pandas
- NumPy
